{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7jAOdoJGQb1b",
        "outputId": "f34efe7a-ccfc-4375-e74e-6afceaed2ca4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iXh9sfIQjJy",
        "outputId": "f6c167a1-cd0d-41f0-ee83-41355f64a5e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from collections import defaultdict\n",
        "\n",
        "# Define device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Load pre-trained BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Define transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load dataset\n",
        "class MSRVTTDataset(Dataset):\n",
        "    def __init__(self, root_dir, metadata_path, split=\"train\", transform=None, num_frames=20):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (str): Path to the root directory containing the \"frames\" folder.\n",
        "            metadata_path (str): Path to the JSON file containing video and caption metadata.\n",
        "            split (str): Dataset split (\"train\", \"val\", or \"test\").\n",
        "            transform (callable, optional): Optional transform to be applied to frames.\n",
        "            num_frames (int): Number of frames to sample from each video.\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.split = split\n",
        "        self.transform = transform\n",
        "        self.num_frames = num_frames\n",
        "\n",
        "        # Load metadata\n",
        "        with open(metadata_path, \"r\") as f:\n",
        "            self.metadata = json.load(f)\n",
        "\n",
        "        # Filter videos by split\n",
        "        self.videos = [video for video in self.metadata[\"videos\"] if video[\"split\"] == split]\n",
        "\n",
        "        # Map video IDs to captions\n",
        "        self.video_to_captions = defaultdict(list)\n",
        "        for sentence in self.metadata[\"sentences\"]:\n",
        "            self.video_to_captions[sentence[\"video_id\"]].append(sentence[\"caption\"])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.videos)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get video metadata\n",
        "        video = self.videos[idx]\n",
        "        video_id = video[\"video_id\"]\n",
        "        frame_folder = os.path.join(self.root_dir, \"frames\", video_id)\n",
        "\n",
        "        # Load frames\n",
        "        frame_files = sorted(os.listdir(frame_folder))\n",
        "        frame_files = [os.path.join(frame_folder, f) for f in frame_files]\n",
        "\n",
        "        # Sample a fixed number of frames\n",
        "        if len(frame_files) > self.num_frames:\n",
        "            frame_files = self._sample_frames(frame_files, self.num_frames)\n",
        "        frames = [Image.open(f) for f in frame_files]\n",
        "\n",
        "        # Pad frames if there are fewer than num_frames\n",
        "        if len(frames) < self.num_frames:\n",
        "            frames = self._pad_frames(frames, self.num_frames)\n",
        "\n",
        "        # Apply transforms (if any)\n",
        "        if self.transform:\n",
        "            frames = [self.transform(frame) for frame in frames]\n",
        "\n",
        "        # Convert frames to tensor\n",
        "        frames = torch.stack(frames)  # Shape: (num_frames, 3, H, W)\n",
        "\n",
        "        # Get captions\n",
        "        captions = self.video_to_captions[video_id]\n",
        "\n",
        "        # Use the first frame as the image input\n",
        "        image_input = frames[0]  # Shape: (3, H, W)\n",
        "\n",
        "        return frames, captions, image_input\n",
        "\n",
        "    def _sample_frames(self, frame_files, num_frames):\n",
        "        \"\"\"\n",
        "        Sample a fixed number of frames from a list of frame files.\n",
        "        \"\"\"\n",
        "        step = len(frame_files) // num_frames\n",
        "        return frame_files[::step][:num_frames]\n",
        "\n",
        "    def _pad_frames(self, frames, num_frames):\n",
        "        \"\"\"\n",
        "        Pad frames with the last frame to ensure a fixed number of frames.\n",
        "        \"\"\"\n",
        "        if len(frames) == 0:\n",
        "            raise ValueError(\"No frames found in the video folder.\")\n",
        "        last_frame = frames[-1]\n",
        "        while len(frames) < num_frames:\n",
        "            frames.append(last_frame)\n",
        "        return frames\n",
        "\n",
        "\n",
        "class VideoGenerationModel(nn.Module):\n",
        "    def __init__(self, text_embedding_size=768, image_embedding_size=2048, hidden_size=512, num_frames=20, frame_size=224):\n",
        "        super(VideoGenerationModel, self).__init__()\n",
        "        self.text_embedding_size = text_embedding_size\n",
        "        self.image_embedding_size = image_embedding_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_frames = num_frames\n",
        "        self.frame_size = frame_size\n",
        "\n",
        "        # Text encoder (BERT embeddings are already encoded, so this is just a linear layer)\n",
        "        self.text_fc = nn.Linear(text_embedding_size, hidden_size)\n",
        "\n",
        "        # Image encoder (ResNet-50)\n",
        "        self.image_encoder = models.resnet50(pretrained=True)\n",
        "        self.image_encoder.fc = nn.Linear(self.image_encoder.fc.in_features, hidden_size)\n",
        "\n",
        "        # Frame generator (GAN)\n",
        "        self.generator = nn.Sequential(\n",
        "            nn.ConvTranspose2d(hidden_size * 2, 1024, kernel_size=4, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(1024),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 3 * num_frames, kernel_size=4, stride=2, padding=1, output_padding=0),  # Adjusted for 224x224 output\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, text_embedding, image_embedding):\n",
        "        # Process text and image embeddings\n",
        "        text_hidden = self.text_fc(text_embedding).unsqueeze(-1).unsqueeze(-1)  # Shape: (batch_size, hidden_size, 1, 1)\n",
        "        image_hidden = self.image_encoder(image_embedding).unsqueeze(-1).unsqueeze(-1)  # Shape: (batch_size, hidden_size, 1, 1)\n",
        "\n",
        "        # Debugging: Print shapes\n",
        "        print(\"Text hidden shape:\", text_hidden.shape)\n",
        "        print(\"Image hidden shape:\", image_hidden.shape)\n",
        "\n",
        "        # Concatenate text and image embeddings\n",
        "        combined_hidden = torch.cat([text_hidden, image_hidden], dim=1)  # Shape: (batch_size, hidden_size * 2, 1, 1)\n",
        "\n",
        "        # Generate frames\n",
        "        frames = self.generator(combined_hidden)  # Shape: (batch_size, 3 * num_frames, H, W)\n",
        "\n",
        "        # Debugging: Print shapes\n",
        "        print(\"Generated frames shape before interpolation:\", frames.shape)\n",
        "\n",
        "        # Interpolate frames to the desired size (224x224)\n",
        "        frames = torch.nn.functional.interpolate(frames, size=(self.frame_size, self.frame_size), mode=\"bilinear\", align_corners=False)\n",
        "\n",
        "        # Debugging: Print shapes\n",
        "        print(\"Generated frames shape after interpolation:\", frames.shape)\n",
        "\n",
        "        # Reshape frames\n",
        "        frames = frames.view(frames.size(0), self.num_frames, 3, self.frame_size, self.frame_size)  # Shape: (batch_size, num_frames, 3, H, W)\n",
        "\n",
        "        return frames"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yC3FeBiXQj-2",
        "outputId": "e03db8d5-d06d-4b7a-d44d-3e7febf01295"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "dataset = MSRVTTDataset(\n",
        "    root_dir=\"/content/drive/MyDrive/msr-vtt\",  # Path to the folder containing the \"frames\" folder\n",
        "    metadata_path=\"/content/drive/MyDrive/msr-vtt/train_val_videodatainfo.json\",  # Path to metadata file\n",
        "    split=\"train\",\n",
        "    transform=transform,\n",
        "    num_frames=20\n",
        ")\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=0)\n",
        "\n",
        "\n",
        "# Initialize models\n",
        "text_encoder = BertModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
        "video_generator = VideoGenerationModel().to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(video_generator.parameters(), lr=1e-4)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    video_generator.train()\n",
        "    for frames, captions, image_input in dataloader:\n",
        "        frames = frames.to(device)  # Move frames to GPU\n",
        "        image_input = image_input.to(device)  # Move image input to GPU\n",
        "\n",
        "        # Flatten captions into a single list of strings\n",
        "        flat_captions = [caption for video_captions in captions for caption in video_captions]\n",
        "\n",
        "        # Tokenize captions\n",
        "        inputs = tokenizer(flat_captions, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "        # Get BERT embeddings\n",
        "        with torch.no_grad():\n",
        "            outputs = text_encoder(**inputs)\n",
        "            text_embeddings = outputs.last_hidden_state[:, 0, :]  # Use [CLS] token\n",
        "\n",
        "        # Reshape text embeddings to match the batch size of image_input\n",
        "        batch_size = image_input.size(0)  # Get the batch size of image_input\n",
        "\n",
        "        num_captions_per_video = text_embeddings.shape[0] // batch_size  # Calculate captions per video dynamically\n",
        "\n",
        "        print(\"Batch size:\", batch_size)\n",
        "        print(\"Num captions per video:\", num_captions_per_video)\n",
        "\n",
        "        # Reshape correctly using .reshape()\n",
        "        text_embeddings = text_embeddings.reshape(batch_size, num_captions_per_video, -1)\n",
        "\n",
        "        # Average across captions\n",
        "        text_embeddings = text_embeddings.mean(dim=1)\n",
        "\n",
        "        print(\"Final text embedding shape:\", text_embeddings.shape)  # Should be (batch_size, 768)\n",
        "\n",
        "        # Debugging: Print shapes\n",
        "        print(\"Text embeddings shape after reshaping:\", text_embeddings.shape)\n",
        "        print(\"Image input shape:\", image_input.shape)\n",
        "\n",
        "        # Generate frames\n",
        "        generated_frames = video_generator(text_embeddings, image_input)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(generated_frames, frames)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7F0bwCEShQg",
        "outputId": "4862814a-18e6-4eb6-f09c-e2d4a9d9dfbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch size: 3\n",
            "Num captions per video: 20\n",
            "Final text embedding shape: torch.Size([3, 768])\n",
            "Text embeddings shape after reshaping: torch.Size([3, 768])\n",
            "Image input shape: torch.Size([3, 3, 224, 224])\n",
            "Text hidden shape: torch.Size([3, 512, 1, 1])\n",
            "Image hidden shape: torch.Size([3, 512, 1, 1])\n",
            "Generated frames shape before interpolation: torch.Size([3, 60, 256, 256])\n",
            "Generated frames shape after interpolation: torch.Size([3, 60, 224, 224])\n",
            "Epoch [1/10], Loss: 1.9987\n",
            "Batch size: 3\n",
            "Num captions per video: 20\n",
            "Final text embedding shape: torch.Size([3, 768])\n",
            "Text embeddings shape after reshaping: torch.Size([3, 768])\n",
            "Image input shape: torch.Size([3, 3, 224, 224])\n",
            "Text hidden shape: torch.Size([3, 512, 1, 1])\n",
            "Image hidden shape: torch.Size([3, 512, 1, 1])\n",
            "Generated frames shape before interpolation: torch.Size([3, 60, 256, 256])\n",
            "Generated frames shape after interpolation: torch.Size([3, 60, 224, 224])\n",
            "Epoch [2/10], Loss: 1.9875\n",
            "Batch size: 3\n",
            "Num captions per video: 20\n",
            "Final text embedding shape: torch.Size([3, 768])\n",
            "Text embeddings shape after reshaping: torch.Size([3, 768])\n",
            "Image input shape: torch.Size([3, 3, 224, 224])\n",
            "Text hidden shape: torch.Size([3, 512, 1, 1])\n",
            "Image hidden shape: torch.Size([3, 512, 1, 1])\n",
            "Generated frames shape before interpolation: torch.Size([3, 60, 256, 256])\n",
            "Generated frames shape after interpolation: torch.Size([3, 60, 224, 224])\n",
            "Epoch [3/10], Loss: 1.9759\n",
            "Batch size: 3\n",
            "Num captions per video: 20\n",
            "Final text embedding shape: torch.Size([3, 768])\n",
            "Text embeddings shape after reshaping: torch.Size([3, 768])\n",
            "Image input shape: torch.Size([3, 3, 224, 224])\n",
            "Text hidden shape: torch.Size([3, 512, 1, 1])\n",
            "Image hidden shape: torch.Size([3, 512, 1, 1])\n",
            "Generated frames shape before interpolation: torch.Size([3, 60, 256, 256])\n",
            "Generated frames shape after interpolation: torch.Size([3, 60, 224, 224])\n",
            "Epoch [4/10], Loss: 1.9648\n",
            "Batch size: 3\n",
            "Num captions per video: 20\n",
            "Final text embedding shape: torch.Size([3, 768])\n",
            "Text embeddings shape after reshaping: torch.Size([3, 768])\n",
            "Image input shape: torch.Size([3, 3, 224, 224])\n",
            "Text hidden shape: torch.Size([3, 512, 1, 1])\n",
            "Image hidden shape: torch.Size([3, 512, 1, 1])\n",
            "Generated frames shape before interpolation: torch.Size([3, 60, 256, 256])\n",
            "Generated frames shape after interpolation: torch.Size([3, 60, 224, 224])\n",
            "Epoch [5/10], Loss: 1.9537\n",
            "Batch size: 3\n",
            "Num captions per video: 20\n",
            "Final text embedding shape: torch.Size([3, 768])\n",
            "Text embeddings shape after reshaping: torch.Size([3, 768])\n",
            "Image input shape: torch.Size([3, 3, 224, 224])\n",
            "Text hidden shape: torch.Size([3, 512, 1, 1])\n",
            "Image hidden shape: torch.Size([3, 512, 1, 1])\n",
            "Generated frames shape before interpolation: torch.Size([3, 60, 256, 256])\n",
            "Generated frames shape after interpolation: torch.Size([3, 60, 224, 224])\n",
            "Epoch [6/10], Loss: 1.9428\n",
            "Batch size: 3\n",
            "Num captions per video: 20\n",
            "Final text embedding shape: torch.Size([3, 768])\n",
            "Text embeddings shape after reshaping: torch.Size([3, 768])\n",
            "Image input shape: torch.Size([3, 3, 224, 224])\n",
            "Text hidden shape: torch.Size([3, 512, 1, 1])\n",
            "Image hidden shape: torch.Size([3, 512, 1, 1])\n",
            "Generated frames shape before interpolation: torch.Size([3, 60, 256, 256])\n",
            "Generated frames shape after interpolation: torch.Size([3, 60, 224, 224])\n",
            "Epoch [7/10], Loss: 1.9318\n",
            "Batch size: 3\n",
            "Num captions per video: 20\n",
            "Final text embedding shape: torch.Size([3, 768])\n",
            "Text embeddings shape after reshaping: torch.Size([3, 768])\n",
            "Image input shape: torch.Size([3, 3, 224, 224])\n",
            "Text hidden shape: torch.Size([3, 512, 1, 1])\n",
            "Image hidden shape: torch.Size([3, 512, 1, 1])\n",
            "Generated frames shape before interpolation: torch.Size([3, 60, 256, 256])\n",
            "Generated frames shape after interpolation: torch.Size([3, 60, 224, 224])\n",
            "Epoch [8/10], Loss: 1.9207\n",
            "Batch size: 3\n",
            "Num captions per video: 20\n",
            "Final text embedding shape: torch.Size([3, 768])\n",
            "Text embeddings shape after reshaping: torch.Size([3, 768])\n",
            "Image input shape: torch.Size([3, 3, 224, 224])\n",
            "Text hidden shape: torch.Size([3, 512, 1, 1])\n",
            "Image hidden shape: torch.Size([3, 512, 1, 1])\n",
            "Generated frames shape before interpolation: torch.Size([3, 60, 256, 256])\n",
            "Generated frames shape after interpolation: torch.Size([3, 60, 224, 224])\n",
            "Epoch [9/10], Loss: 1.9094\n",
            "Batch size: 3\n",
            "Num captions per video: 20\n",
            "Final text embedding shape: torch.Size([3, 768])\n",
            "Text embeddings shape after reshaping: torch.Size([3, 768])\n",
            "Image input shape: torch.Size([3, 3, 224, 224])\n",
            "Text hidden shape: torch.Size([3, 512, 1, 1])\n",
            "Image hidden shape: torch.Size([3, 512, 1, 1])\n",
            "Generated frames shape before interpolation: torch.Size([3, 60, 256, 256])\n",
            "Generated frames shape after interpolation: torch.Size([3, 60, 224, 224])\n",
            "Epoch [10/10], Loss: 1.8980\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(video_generator.state_dict(), \"/content/drive/MyDrive/video_generator.pth\")"
      ],
      "metadata": {
        "id": "qykKJ794fmku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Define device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the trained model\n",
        "video_generator = VideoGenerationModel().to(device)\n",
        "video_generator.load_state_dict(torch.load(\"/content/drive/MyDrive/video_generator.pth\"))\n",
        "video_generator.eval()\n",
        "\n",
        "# Define transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Load the text encoder (BERT)\n",
        "text_encoder = BertModel.from_pretrained(\"bert-base-uncased\").to(device)\n",
        "\n",
        "# Load an input image\n",
        "image_path = \"/content/drive/MyDrive/msr-vtt/inputCartoon.jpeg\"  # Replace with the path to your input image\n",
        "image = Image.open(image_path).convert(\"RGB\")\n",
        "image = transform(image).unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
        "\n",
        "# Define a text caption\n",
        "text = \"a cartoon character runs through an ice cave\"  # Replace with your desired caption\n",
        "\n",
        "# Tokenize the text caption\n",
        "inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "# Get BERT embeddings\n",
        "with torch.no_grad():\n",
        "    outputs = text_encoder(**inputs)\n",
        "    text_embedding = outputs.last_hidden_state[:, 0, :]  # Use [CLS] token\n",
        "\n",
        "# Generate frames\n",
        "with torch.no_grad():\n",
        "    generated_frames = video_generator(text_embedding, image)\n",
        "\n",
        "# Convert frames to numpy array\n",
        "frames = generated_frames.squeeze().cpu().numpy()  # Shape: (num_frames, 3, H, W)\n",
        "frames = np.transpose(frames, (0, 2, 3, 1))  # Shape: (num_frames, H, W, 3)\n",
        "frames = (frames * 255).astype(np.uint8)  # Convert to uint8\n",
        "\n",
        "# Save frames as a video\n",
        "output_video_path = \"/content/drive/MyDrive/generated_video.mp4\"  # Replace with your desired output path\n",
        "fps = 10  # Frames per second\n",
        "frame_size = (224, 224)  # Frame size\n",
        "\n",
        "# Create a video writer\n",
        "out = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, frame_size)\n",
        "\n",
        "# Write frames to the video\n",
        "for frame in frames:\n",
        "    out.write(frame)\n",
        "\n",
        "# Release the video writer\n",
        "out.release()\n",
        "\n",
        "print(f\"Video saved to {output_video_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTGU4eWATNRX",
        "outputId": "2dec548c-907d-4b60-a2c9-5f1b961c5292"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text hidden shape: torch.Size([1, 512, 1, 1])\n",
            "Image hidden shape: torch.Size([1, 512, 1, 1])\n",
            "Generated frames shape before interpolation: torch.Size([1, 60, 256, 256])\n",
            "Generated frames shape after interpolation: torch.Size([1, 60, 224, 224])\n",
            "Video saved to /content/drive/MyDrive/generated_video.mp4\n"
          ]
        }
      ]
    }
  ]
}